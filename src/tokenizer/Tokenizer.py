import nltk
#from src.tokenizer import sandisplitter

"""
This is the old code - please remove if not required.

a = nltk.word_tokenize(new)
print(a)
"""

def doTokenization(filmReviewList):
    # the parameter - filmReviewList, recieved here is the reviewList selected by user
    # function should cordinate with sandi splitter and return a tokenized list
    print("  1.Before Tokenization - ", filmReviewList)
    
    #tokenize words (split by whitespace)
    wordListOfReviewList = []
    for review in filmReviewList:
        wordListOfReview = getWordList(review)
        wordListOfReviewList.append(wordListOfReview)
    print("  2.Tokenize by whitespace - ", wordListOfReviewList)
    
    # replace complex complex words with its fragments generated by sandiplitter
    for wordListOfReview in wordListOfReviewList: 
        wordIndex = 0
        for word in wordListOfReview:
            if isComplexWord(word):
                fragmentWordList = breakComplexWords(word)
                #replace complex word with its fragment words 
                wordListOfReview[wordIndex] = fragmentWordList
            wordIndex += 1  #increment
    print("  3.Replace complex words [to be implemented]- ", wordListOfReviewList)
    return wordListOfReviewList

def getWordList(sentence):
    tokenList = nltk.word_tokenize(sentence)
    return tokenList
    
    
def breakComplexWords(word):
    # to implement
    # do call the sandisplitter module here
    # sandisplitter module should return a list of words equivalent to a complex word
    x=1
    
    
def isComplexWord(word):
    # to implement
    # do call the sandisplitter module here
    # sandisplitter module's method should return a boolean 
    return False